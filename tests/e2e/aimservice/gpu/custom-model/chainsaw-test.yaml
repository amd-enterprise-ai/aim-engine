# yaml-language-server: $schema=https://raw.githubusercontent.com/kyverno/chainsaw/main/.schemas/json/test-chainsaw-v1alpha1.json
#
# AIMService E2E Test: Custom Model with GPU (GPU)
#
# This test validates AIMService with spec.model.custom on GPU infrastructure.
# The custom model specification allows deploying models without pre-created
# AIMModel resources - the controller auto-creates all necessary resources.
#
# This test validates:
# 1. AIMModel is auto-created with correct modelSources and hardware
# 2. AIMServiceTemplate is auto-created from hardware requirements
# 3. Dedicated model caches are created for model download
# 4. InferenceService is created with correct GPU configuration
# 5. Pod has correct AIM_MODEL_ID env var from modelSources
# 6. Pod has correct container image from baseImage
# 7. Pod has correct GPU resources
# 8. Service reaches Running status
# 9. Chat completions endpoint works correctly
#
# Uses: amdenterpriseai/aim-base:0.9 + HuggingFace model
# Requires: GPU infrastructure
#
apiVersion: chainsaw.kyverno.io/v1alpha1
kind: Test
metadata:
  name: aimservice-gpu-custom-model
  labels:
    requires: gpu
spec:
  description: Test AIMService with custom model specification on GPU
  timeouts:
    assert: 600s
  steps:
    - name: Create runtime config with routing
      try:
        - apply:
            file: runtime-config.yaml

    - name: Create service with custom model
      try:
        - apply:
            file: service.yaml

    - name: Verify service starts
      try:
        - assert:
            timeout: 60s
            resource:
              apiVersion: aim.eai.amd.com/v1alpha1
              kind: AIMService
              metadata:
                name: test-gpu-custom-model
              status:
                status: Starting

    - name: Verify AIMModel is auto-created for custom model
      try:
        - assert:
            timeout: 120s
            resource:
              apiVersion: aim.eai.amd.com/v1alpha1
              kind: AIMModel
              metadata:
                labels:
                  aim.eai.amd.com/origin: auto-generated
                  aim.eai.amd.com/custom-model: "true"
              spec:
                modelSources:
                  - modelId: Qwen/Qwen2-0.5B
                    sourceUri: hf://Qwen/Qwen2-0.5B
              status:
                status: Ready
                sourceType: Custom

    - name: Verify AIMServiceTemplate is auto-created from hardware spec
      try:
        - assert:
            timeout: 120s
            resource:
              apiVersion: aim.eai.amd.com/v1alpha1
              kind: AIMServiceTemplate
              metadata:
                labels:
                  aim.eai.amd.com/origin: auto-generated
              status:
                status: Ready

    - name: Verify model cache is created via template cache
      try:
        - assert:
            timeout: 120s
            resource:
              apiVersion: aim.eai.amd.com/v1alpha1
              kind: AIMModelCache
              metadata:
                labels:
                  aim.eai.amd.com/service: test-gpu-custom-model

    - name: Verify InferenceService is created with GPU resources
      try:
        - assert:
            timeout: 300s
            resource:
              apiVersion: serving.kserve.io/v1beta1
              kind: InferenceService
              metadata:
                labels:
                  aim.eai.amd.com/service: test-gpu-custom-model

    - name: Verify Pod has correct env vars, image, and GPU resources
      try:
        - script:
            timeout: 5m
            env:
              - name: NAMESPACE
                value: ($namespace)
            content: |
              set -e

              # Wait for predictor pod to be running
              echo "Waiting for predictor pod..."
              for i in $(seq 1 60); do
                POD=$(kubectl get pods -n "$NAMESPACE" -l serving.kserve.io/inferenceservice=test-gpu-custom-model -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
                if [ -n "$POD" ]; then
                  STATUS=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.status.phase}' 2>/dev/null || true)
                  if [ "$STATUS" = "Running" ]; then
                    echo "Found running pod: $POD"
                    break
                  fi
                fi
                sleep 5
              done

              if [ -z "$POD" ]; then
                echo "ERROR: No predictor pod found"
                exit 1
              fi

              # Get the kserve-container spec
              CONTAINER_JSON=$(kubectl get pod "$POD" -n "$NAMESPACE" -o jsonpath='{.spec.containers[?(@.name=="kserve-container")]}')

              # Verify container image matches baseImage
              IMAGE=$(echo "$CONTAINER_JSON" | jq -r '.image')
              echo "Container image: $IMAGE"
              if [[ "$IMAGE" != *"aim-base"* ]]; then
                echo "ERROR: Expected aim-base image, got: $IMAGE"
                exit 1
              fi
              echo "SUCCESS: Container image is correct"

              # Verify AIM_MODEL_ID env var
              AIM_MODEL_ID=$(echo "$CONTAINER_JSON" | jq -r '.env[] | select(.name=="AIM_MODEL_ID") | .value')
              echo "AIM_MODEL_ID: $AIM_MODEL_ID"
              if [ "$AIM_MODEL_ID" != "Qwen/Qwen2-0.5B" ]; then
                echo "ERROR: Expected AIM_MODEL_ID=Qwen/Qwen2-0.5B, got: $AIM_MODEL_ID"
                exit 1
              fi
              echo "SUCCESS: AIM_MODEL_ID is correct"

              # Verify GPU resources
              GPU_REQUESTS=$(echo "$CONTAINER_JSON" | jq -r '.resources.requests["amd.com/gpu"] // .resources.limits["amd.com/gpu"] // "0"')
              echo "GPU requests: $GPU_REQUESTS"
              if [ "$GPU_REQUESTS" = "0" ] || [ "$GPU_REQUESTS" = "null" ]; then
                echo "ERROR: Expected GPU resources to be set"
                exit 1
              fi
              echo "SUCCESS: GPU resources are configured"

              echo "All Pod-level assertions passed!"

    - name: Verify service reaches Running status
      try:
        - assert:
            timeout: 10m
            resource:
              apiVersion: aim.eai.amd.com/v1alpha1
              kind: AIMService
              metadata:
                name: test-gpu-custom-model
              status:
                status: Running

    - name: Verify chat completions endpoint works
      try:
        - script:
            timeout: 3m
            env:
              - name: HTTP_BASE_PATH
                value: /integration/gpu-custom-model/test-gpu-custom-model/v1
            content: bash ../../_shared/validate-http.sh
